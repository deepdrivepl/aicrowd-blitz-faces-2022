{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import timm # !pip install git+https://github.com/rwightman/pytorch-image-models\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchmetrics.functional.classification.accuracy import accuracy\n",
    "from torchmetrics.functional import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_erase(pilimg):\n",
    "    img = np.array(pilimg)\n",
    "\n",
    "    h,w,_ = img.shape\n",
    "\n",
    "    xx = np.random.random(2)\n",
    "    xmin = int(min(xx)*w)\n",
    "    xmax = int(max(xx)*w)\n",
    "\n",
    "    yy = np.random.random(2)\n",
    "    ymin = int(min(yy)*h)\n",
    "    ymax = int(max(yy)*h)\n",
    "\n",
    "    # print(ymin,ymax,xmin,xmax)\n",
    "\n",
    "    img[ymin:ymax,xmin:xmax,:]=0\n",
    "\n",
    "    return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, path, transform, erase=False):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.path = path\n",
    "        \n",
    "        self.erase=erase\n",
    "\n",
    "\n",
    "\n",
    "        self.Ages = ['0-10',\n",
    " '10-20',\n",
    " '20-30',\n",
    " '30-40',\n",
    " '40-50',\n",
    " '50-60',\n",
    " '60-70',\n",
    " '70-80',\n",
    " '80-90',\n",
    " '90-100']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.path + '/' + self.dataframe.iloc[idx, 0] + '.jpg'\n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "        age = self.Ages.index(self.dataframe.iloc[idx, 1])\n",
    "        age = torch.Tensor(np.array([age]).reshape(-1)).long()\n",
    "        \n",
    "        if self.erase:\n",
    "            image = random_erase(image)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = {'image': image, 'age': age}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def get_counts(self):\n",
    "        return collections.defaultdict(int, self.dataframe.age.value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ordinal_regression(predictions, targets):\n",
    "    \"\"\"Ordinal regression with encoding as in https://arxiv.org/pdf/0704.1028.pdf\"\"\"\n",
    "\n",
    "    # Create out modified target with [batch_size, num_labels] shape\n",
    "    modified_target = torch.zeros_like(predictions)\n",
    "\n",
    "    # Fill in ordinal target function, i.e. 0 -> [1,0,0,...]\n",
    "    for i, target in enumerate(targets):\n",
    "        #modified_target[i, 0:target] = 1\n",
    "        modified_target[i, 0:target+1] = 1\n",
    "\n",
    "        \n",
    "    mse = nn.MSELoss(reduction='mean')(predictions, modified_target)\n",
    "\n",
    "    \n",
    "#     mse = nn.MSELoss(reduction='none')(predictions, modified_target)\n",
    "#     weights=torch.Tensor([1.0,0.3,0.3,0.3,1.0]).type_as(mse)\n",
    "#     mse = torch.mean(torch.mul(mse, weights))\n",
    "\n",
    "    return mse\n",
    "\n",
    "def prediction2label(pred: np.ndarray):\n",
    "    \"\"\"Convert ordinal predictions to class labels, e.g.\n",
    "    \n",
    "    [0.9, 0.1, 0.1, 0.1] -> 0\n",
    "    [0.9, 0.9, 0.1, 0.1] -> 1\n",
    "    [0.9, 0.9, 0.9, 0.1] -> 2\n",
    "    etc.\n",
    "    \"\"\"\n",
    "    return torch.clamp((pred > 0.5).cumprod(axis=1).sum(axis=1,keepdims=True)-1, min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def balanced_accuracy(y_true, y_pred, sample_weight=None):\n",
    "#     y_true = y_true.detach().cpu().numpy().astype(int) # compile model with run_eagerly=True\n",
    "#     y_pred = np.argmax(y_pred.detach().cpu().numpy(),axis=1).astype(int)\n",
    "#     return balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getModel(backbone, train_size):\n",
    "    model = nn.ModuleList([timm.create_model(backbone, pretrained=True, num_classes=0),])\n",
    "    in_features = model[0](torch.rand(1,3,train_size, train_size)).shape[-1]\n",
    "\n",
    "    model.append(nn.Sequential(torch.nn.LayerNorm([in_features, ]),\n",
    "                          nn.Dropout(0.5),\n",
    "                          nn.Linear(in_features, 128),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Linear(128, 10),\n",
    "                          nn.Sigmoid()))\n",
    "                              \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeClassification(LightningModule):\n",
    "    def __init__(self, net, train_size, test_size, transform_train, transform_val, batch_size=16, lr=1e-3,\n",
    "                pt_fname=None, ckpt_fname=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters('batch_size', 'lr', 'train_size', 'test_size')\n",
    "\n",
    "        self.train_csv = 'data/train.csv'\n",
    "        self.val_csv = 'data/val.csv'\n",
    "        self.classes = ['0-10', '20-30', '40-50', '70-80', '80-90', '30-40', '50-60', '10-20', '60-70', '90-100']\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.test_size = test_size\n",
    "\n",
    "        self.example_input_array = torch.rand(1, 3, self.train_size, self.train_size)\n",
    "\n",
    "        self.transform_train = transform_train\n",
    "        self.transform_val = transform_val\n",
    "        \n",
    "        self.setup()\n",
    "        \n",
    "        self.loss=ordinal_regression        \n",
    "        self.lr = lr\n",
    "        self.net = net\n",
    "\n",
    "    def custom_histogram_adder(self):\n",
    "        for name,params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name,params,self.current_epoch)\n",
    "    \n",
    "    def freeze(self, epochs):\n",
    "        for i,param in enumerate(self.net[0].parameters()):\n",
    "            param.requires_grad = False\n",
    "        for i,param in enumerate(self.net[1].parameters()):\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def unfreeze(self, epochs):\n",
    "        for i,param in enumerate(self.net[0].parameters()):\n",
    "            param.requires_grad = True\n",
    "        for i,param in enumerate(self.net[1].parameters()):\n",
    "            param.requires_grad = True\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def freeze_bn(self):\n",
    "        for module in self.modules():\n",
    "            # print(module)\n",
    "            if isinstance(module, nn.BatchNorm1d) or isinstance(module, nn.BatchNorm2d):\n",
    "                # print(module,'==> FREEZE!!!')\n",
    "                if hasattr(module, 'weight'):\n",
    "                    module.weight.requires_grad_(False)\n",
    "                if hasattr(module, 'bias'):\n",
    "                    module.bias.requires_grad_(False)\n",
    "                module.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net[0](x)\n",
    "        results = [fc(x) for fc in self.net[1:]]\n",
    "        return tuple(results)\n",
    "    # def load_from_ckpt(self, fname):\n",
    "    #     self.load_state_dict(fname, strict=False)\n",
    "\n",
    "    \n",
    "    def on_fit_start(self):\n",
    "        self.logger.log_hyperparams(self.hparams,\n",
    "                                    metrics=#{})\n",
    "        dict.fromkeys(['hp/val/acc/age'], float('NaN')))\n",
    "    \n",
    "    def _step(self, mode, batch, batch_idx):\n",
    "        x = batch['image']\n",
    "        y = batch['age']\n",
    "        \n",
    "        all_logits = self(x)\n",
    "        \n",
    "        loss = self.loss(all_logits[0], y)\n",
    "\n",
    "        acc = accuracy(prediction2label(all_logits[0]), y.long())\n",
    "#         acc = accuracy(prediction2label(all_logits[0]).detach(), y.long())\n",
    "        \n",
    "        d = {}\n",
    "        \n",
    "        if mode=='val':\n",
    "            l = all_logits[0]#.detach()#.cpu()\n",
    "            n = 'age'\n",
    "            d['val/%s/target'%n]=y\n",
    "            d['val/%s/pred'%n]=prediction2label(l)\n",
    "                \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(mode+'/loss/age', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(mode+'/acc/age', acc, on_step=False, on_epoch=True, prog_bar=(mode=='val'), logger=True)\n",
    "\n",
    "        # d[mode+'_loss']+=loss\n",
    "        d['loss'] = loss\n",
    "\n",
    "        d['loss_age'] = loss\n",
    "        d['acc_age'] = acc#.detach()\n",
    "        \n",
    "        return d\n",
    "    def training_epoch_end(self, outputs):\n",
    "        # logging histograms\n",
    "        self.custom_histogram_adder()\n",
    "\n",
    "    def validation_step_end(self, batch_parts):\n",
    "        pass\n",
    "#         # DP - size=0 problem https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#validating-with-dataparallel\n",
    "#         print('\\n\\nvalidation_step_end',type(batch_parts),len(batch_parts))\n",
    "#         print('\\n')\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "#         print('validation_epoch_end len=%d'%(len(outputs)))#, outputs)\n",
    "#         print('='*80)\n",
    "#         for output in outputs:\n",
    "#             for k in output:\n",
    "# #                 print(k, output[k], output[k].shape)\n",
    "#                 print(k, output[k].shape)\n",
    "#             print('-'*80)\n",
    "        if len(outputs)==0:\n",
    "            return\n",
    "        fig = plt.figure(figsize=(30,20))\n",
    "        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "        preds = torch.cat([tmp['val/age/pred'] for tmp in outputs],0)\n",
    "        targets = torch.cat([tmp['val/age/target'] for tmp in outputs],0)\n",
    "\n",
    "        cm = confusion_matrix(preds.cpu(), targets.cpu(), num_classes=len(self.classes),\n",
    "                              normalize='true').cpu()\n",
    "        #https://torchmetrics.readthedocs.io/en/stable/references/functional.html#confusion-matrix-func\n",
    "\n",
    "        df_cm = pd.DataFrame(cm.numpy(), index = self.classes, columns=self.classes)\n",
    "        sns.heatmap(df_cm, annot=True, fmt='.2f',cmap='Oranges')\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        age_valacc = torch.stack([x['acc_age'] for x in outputs]).mean()\n",
    "        self.log('hp/val/acc/age', age_valacc)\n",
    "\n",
    "        \n",
    "        self.logger.experiment.add_figure('val/cm', fig, self.current_epoch)\n",
    "        if self.current_epoch==0:\n",
    "            self.plot_train_examples()\n",
    "            self.plot_val_examples()\n",
    "            self.plot_counts()\n",
    "            self.plot_val_counts()\n",
    "            self.log('train/samples/total', len(self.train_dataset))\n",
    "            self.log('val/samples/total', len(self.val_dataset))\n",
    "#             for i in range(len(self.counts)):\n",
    "#                 for j in range(len(self.classes[i])):\n",
    "#                     print('train/samples/%s'%self.classes[i][j],self.counts[i][j])\n",
    "#                     self.log('train/samples/%s'%self.classes[i][j],self.counts[i][j])\n",
    "#             for i in range(len(self.val_counts)):\n",
    "#                 for j in range(len(self.classes[i])):\n",
    "#                     self.log('val/samples/%s'%self.classes[i][j],self.val_counts[i][j])\n",
    "\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = AgeDataset(self.train_csv, 'data/train', transform=self.transform_train, erase=True)\n",
    "        self.val_dataset = AgeDataset(self.val_csv, 'data/val', transform=self.transform_val, erase=False)\n",
    "\n",
    "        self.counts = self.train_dataset.get_counts()\n",
    "        self.val_counts = self.val_dataset.get_counts()\n",
    "        \n",
    "        print('self.counts',self.counts)\n",
    "        print('self.val_counts',self.val_counts)\n",
    "\n",
    "    def plot_counts(self):\n",
    "        fig = plt.figure(figsize=(20,6))\n",
    "        print(self.classes, [self.counts[_] for _ in  self.classes])\n",
    "        plt.bar(self.classes, [self.counts[_] for _ in  self.classes])\n",
    "        plt.close(fig)\n",
    "        self.logger.experiment.add_figure('train/dataset/counts', fig, 0)\n",
    "    \n",
    "    def plot_val_counts(self):\n",
    "        fig = plt.figure(figsize=(20,6))\n",
    "        print(self.classes, [self.val_counts[_] for _ in  self.classes])\n",
    "        plt.bar(self.classes, [self.val_counts[_] for _ in  self.classes])\n",
    "        plt.close(fig)\n",
    "        self.logger.experiment.add_figure('val/dataset/counts', fig, 0)\n",
    "    \n",
    "    def plot_examples(self, ds):\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "        for i,b in enumerate(ds):\n",
    "            plt.subplot(3,3,i+1)\n",
    "            img = UnNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))(b['image'])\n",
    "            plt.imshow(img.permute(1,2,0))\n",
    "            plt.axis('off')\n",
    "            t = dict(b)\n",
    "            del t['image']\n",
    "            # plt.title(', '.join([self.classes[i][int(v)] for i,v in enumerate(t.values())]))\n",
    "            for k in t:\n",
    "                t[k] = str(t[k].cpu().numpy().tolist())\n",
    "            plt.title('\\n'.join([\"%s %s\"%(k, t[k])for k in t]))\n",
    "            if i>=8:\n",
    "                break\n",
    "        return fig\n",
    "    \n",
    "    def plot_train_examples(self):\n",
    "        fig = self.plot_examples(self.train_dataset)\n",
    "        plt.suptitle('Training samples')\n",
    "        if self.logger is not None:\n",
    "            plt.close(fig)\n",
    "            self.logger.experiment.add_figure('train/dataset/samples', fig, 0)\n",
    "        \n",
    "    def plot_val_examples(self):\n",
    "        fig = self.plot_examples(self.val_dataset)\n",
    "        plt.suptitle('Validation samples')\n",
    "        if self.logger is not None:\n",
    "            plt.close(fig)\n",
    "            self.logger.experiment.add_figure('val/dataset/samples', fig, 0)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._step('train', batch, batch_idx)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            output = self._step('val', batch, batch_idx)\n",
    "            return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        d=dict()\n",
    "        d[\"optimizer\"] = torch.optim.RAdam(self.parameters(), lr=self.lr)\n",
    "#         d[\"optimizer\"] = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "        print(\"len(self.train_dataset):\",len(self.train_dataset))\n",
    "        print(\"self.batch_size:\",self.batch_size)\n",
    "        total_steps = self.epochs * len(self.train_dataset)//self.batch_size\n",
    "        print(\"total_steps:\",total_steps) #drop last = True\n",
    "\n",
    "        pct_start = 0.1\n",
    "        div_factor = 100\n",
    "        final_div_factor = 100\n",
    "        d[\"lr_scheduler\"] = {\"scheduler\": torch.optim.lr_scheduler.OneCycleLR(d['optimizer'],\n",
    "                                            max_lr=self.lr,\n",
    "                                            total_steps=total_steps,\n",
    "                                            pct_start=pct_start,\n",
    "                                            div_factor=div_factor,\n",
    "                                            final_div_factor=final_div_factor),\n",
    "                            \"frequency\": 1,\n",
    "                            'interval': 'step'\n",
    "                            }\n",
    "        return d\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=0, shuffle=True,drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=0)\n",
    "\n",
    "    def predict_batch_exp(self, batch):\n",
    "        x = self.net[0](batch)\n",
    "        y = [_(x) for _ in self.net[1:]]\n",
    "        return prediction2label(y[0]), torch.nn.functional.softmax(y[1]), torch.nn.functional.softmax(y[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch             : 1.10.2\n",
      "CUDA devices count: 1\n",
      "1st dev name      : GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count()>0:\n",
    "    print(\"Torch             :\",torch.__version__)\n",
    "    print(\"CUDA devices count:\",torch.cuda.device_count())\n",
    "    print(\"1st dev name      :\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"\\n\\nNO CUDA!!!\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_freezed = 256\n",
    "batch_size_unfreezed = 64\n",
    "train_size=384\n",
    "test_size=384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "augmentations = A.Compose(\n",
    "                [A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.5, rotate_limit=20, p=0.8),\n",
    "                A.RGBShift(r_shift_limit=40, g_shift_limit=40, b_shift_limit=40, p=0.9),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.OneOf([\n",
    "                    A.OpticalDistortion(p=0.5),\n",
    "                    A.GridDistortion(p=.5),\n",
    "                    A.PiecewiseAffine(p=0.5),\n",
    "                ], p=0.5),\n",
    "                A.OneOf([\n",
    "                    A.CLAHE(clip_limit=2),\n",
    "                    A.Sharpen(),\n",
    "                    A.Emboss(),\n",
    "                    A.RandomBrightnessContrast(),\n",
    "                ], p=0.5),\n",
    "                A.OneOf([A.Solarize(),\n",
    "                         A.Superpixels(),\n",
    "                         A.Posterize()]),\n",
    "                A.RandomGamma(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.RandomRain(brightness_coefficient=0.9, drop_width=1, blur_value=5, p=0.3),\n",
    "                A.ImageCompression(quality_lower=20, quality_upper=90,p=0.5)\n",
    "                ])\n",
    "\n",
    "transform_train = transforms.Compose([transforms.Resize([train_size, train_size]),\n",
    "                              transforms.Lambda(lambda img: np.array(img)),\n",
    "                              transforms.Lambda(lambda img: augmentations(image=img)['image']),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "                                     ])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize([test_size, test_size]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone='xcit_tiny_12_p8_384_dist'\n",
    "net = getModel(backbone, train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freezed_epochs = 100\n",
    "unfreezed_epochs = 10\n",
    "lr_freezed = 0.03\n",
    "lr_unfreezed = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.counts defaultdict(<class 'int'>, {'90-100': 736, '0-10': 401, '50-60': 385, '80-90': 374, '20-30': 361, '60-70': 357, '40-50': 355, '10-20': 350, '30-40': 342, '70-80': 339})\n",
      "self.val_counts defaultdict(<class 'int'>, {'90-100': 351, '70-80': 202, '60-70': 201, '30-40': 194, '80-90': 191, '40-50': 189, '0-10': 171, '10-20': 168, '20-30': 167, '50-60': 166})\n",
      "self.counts defaultdict(<class 'int'>, {'90-100': 736, '0-10': 401, '50-60': 385, '80-90': 374, '20-30': 361, '60-70': 357, '40-50': 355, '10-20': 350, '30-40': 342, '70-80': 339})\n",
      "self.val_counts defaultdict(<class 'int'>, {'90-100': 351, '70-80': 202, '60-70': 201, '30-40': 194, '80-90': 191, '40-50': 189, '0-10': 171, '10-20': 168, '20-30': 167, '50-60': 166})\n",
      "len(self.train_dataset): 4000\n",
      "self.batch_size: 256\n",
      "total_steps: 1562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_jit_internal.py:668: LightningDeprecationWarning: The `LightningModule.datamodule` property is deprecated in v1.3 and will be removed in v1.5. Access the datamodule through using `self.trainer.datamodule` instead.\n",
      "  if hasattr(mod, name):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_jit_internal.py:668: LightningDeprecationWarning: The `LightningModule.loaded_optimizer_states_dict` property is deprecated in v1.4 and will be removed in v1.6.\n",
      "  if hasattr(mod, name):\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\timm\\models\\xcit.py:271: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 4, 1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\timm\\models\\cait.py:92: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  q = self.q(x[:, 0]).unsqueeze(1).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\timm\\models\\cait.py:93: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\timm\\models\\cait.py:96: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
      "\n",
      "  | Name | Type       | Params | In sizes | Out sizes\n",
      "-----------------------------------------------------------\n",
      "0 | net  | ModuleList | 6.5 M  | ?        | ?        \n",
      "-----------------------------------------------------------\n",
      "26.4 K    Trainable params\n",
      "6.5 M     Non-trainable params\n",
      "6.5 M     Total params\n",
      "26.160    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Roaming\\Python\\Python37\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-10', '20-30', '40-50', '70-80', '80-90', '30-40', '50-60', '10-20', '60-70', '90-100'] [401, 361, 355, 339, 374, 342, 385, 350, 357, 736]\n",
      "['0-10', '20-30', '40-50', '70-80', '80-90', '30-40', '50-60', '10-20', '60-70', '90-100'] [171, 167, 189, 202, 191, 194, 166, 168, 201, 351]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Roaming\\Python\\Python37\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "C:\\Users\\karol\\AppData\\Roaming\\Python\\Python37\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:323: UserWarning: The number of training samples (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34091dbbf6f449cfad5f8cded436b04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Roaming\\Python\\Python37\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:398: LightningDeprecationWarning: One of the returned values {'loss_age', 'acc_age'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  f\"One of the returned values {set(extra.keys())} has a `grad_fn`. We will detach it automatically\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-10', '20-30', '40-50', '70-80', '80-90', '30-40', '50-60', '10-20', '60-70', '90-100'] [401, 361, 355, 339, 374, 342, 385, 350, 357, 736]\n",
      "['0-10', '20-30', '40-50', '70-80', '80-90', '30-40', '50-60', '10-20', '60-70', '90-100'] [171, 167, 189, 202, 191, 194, 166, 168, 201, 351]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "age_model = AgeClassification(net, train_size=train_size, test_size=test_size,\n",
    "                                transform_train=transform_train,\n",
    "                                transform_val=transform_val,\n",
    "                                lr=lr_freezed,\n",
    "                                batch_size=batch_size_freezed)\n",
    "\n",
    "\n",
    "age_model.freeze(freezed_epochs)\n",
    "lr_monitor = pytorch_lightning.callbacks.LearningRateMonitor(logging_interval='step',\n",
    "                                                            log_momentum=True)\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=backbone, default_hp_metric=False, log_graph=True)\n",
    "\n",
    "trainer = Trainer(gpus=1,\n",
    "                  max_epochs=freezed_epochs,\n",
    "                  amp_backend='native',\n",
    "                  precision=16,\n",
    "                  callbacks=[lr_monitor],\n",
    "                  logger=logger)\n",
    "trainer.fit(age_model)\n",
    "del trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_model.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_model.lr = lr_unfreezed\n",
    "age_model.unfreeze(unfreezed_epochs)\n",
    "\n",
    "age_model.batch_size = batch_size_unfreezed\n",
    "\n",
    "ckpt = checkpoint_callback = pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint(monitor='val/acc/age',\n",
    "                                             mode='max',\n",
    "                                             save_top_k=1,\n",
    "                                             verbose=False,\n",
    "                                             filename=model_name+'-epoch{epoch:02d}-age-{val/wacc/age:.2f}',\n",
    "                                             auto_insert_metric_name=False)\n",
    "\n",
    "\n",
    "lr_monitor = pytorch_lightning.callbacks.LearningRateMonitor(logging_interval='step',\n",
    "                                                             log_momentum=True)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=model_name, default_hp_metric=False, log_graph=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=unfreezed_epochs,\n",
    "    amp_backend='native',\n",
    "    precision=16,\n",
    "    callbacks=[ckpt, lr_monitor],\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(age_model)\n",
    "del trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
